{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('Continuum': virtualenv)",
   "metadata": {
    "interpreter": {
     "hash": "7a718d346fe16385aa4dd06ca5d29857e7a0d954ea187236c736d2fd5f7a7a48"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Data Cleaning\n",
    "\n",
    "This script loads raw data from the Building-Data-Genome-Project-2 and performs data Cleaning."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path definition & Parameter Selection\n",
    "meter_data = [\"electricity\", \"gas\", \"hotwater\", \"chilledwater\"]\n",
    "weather_cols = [\"airTemperature\", \"windSpeed\", \"cloudCoverage\"]\n",
    "\n",
    "url_root = 'https://media.githubusercontent.com/media/buds-lab/building-data-genome-project-2/master/data/'\n",
    "path_meters = \"meters/raw/\"\n",
    "path_meta = \"metadata/\"\n",
    "path_weather = \"weather/\"\n",
    "\n",
    "path_data_out = \"..\\\\data\\\\\"\n",
    "path_fig_out = \"..\\\\figures\\\\\"\n",
    "\n",
    "meter_files = [url_root + path_meters + meter+ \".csv\" for meter in meter_data]"
   ]
  },
  {
   "source": [
    "### Reading\n",
    "\n",
    "Dataframes are organized in a multicolumn fashion {meter, building_id}"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_upperlevel_column(df, upperlevel_column_name):\n",
    "    \"\"\"\"A function to define an upper level column over a dataframe.\"\"\"\n",
    "    lowerlevel_column_name = df.columns\n",
    "    tuple_column = []\n",
    "    for i in lowerlevel_column_name:\n",
    "        tuple_column.append((upperlevel_column_name, i))\n",
    "    df.columns = pd.MultiIndex.from_tuples(tuple_column)\n",
    "    return df\n",
    "\n",
    "# Meter data\n",
    "dfs = [] # empty list of the dataframes to create\n",
    "for file in meter_files:\n",
    "    meter_type = file.split(\"/\")[10].split(\".\")[0] # meter_type to rename the value feature\n",
    "    meter = pd.read_csv(file) # load the dataset\n",
    "    meter = meter.set_index(\"timestamp\")\n",
    "    # Define multicolumn Dataframe\n",
    "    meter = set_upperlevel_column(meter, meter_type)\n",
    "    dfs.append(meter)  # append to list\n",
    "df_meter = pd.concat(dfs, axis=1) # concatenate all meter\n",
    "del(dfs, meter, file, meter_files, meter_type)\n",
    "df_meter.index = pd.to_datetime(df_meter.index, format='%Y-%m-%d %H:%M:%S')\n",
    "df_meter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'https://media.githubusercontent.com/media/buds-lab/building-data-genome-project-2/master/data//weatherweather.csv'"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "url_root + path_weather + \"weather.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                  timestamp  site_id  airTemperature  cloudCoverage  windSpeed\n",
       "0       2016-01-01 00:00:00  Panther            19.4            NaN        0.0\n",
       "1       2016-01-01 01:00:00  Panther            21.1            6.0        0.0\n",
       "2       2016-01-01 02:00:00  Panther            21.1            NaN        1.5\n",
       "3       2016-01-01 03:00:00  Panther            20.6            NaN        0.0\n",
       "4       2016-01-01 04:00:00  Panther            21.1            NaN        1.5\n",
       "...                     ...      ...             ...            ...        ...\n",
       "331161  2017-12-31 19:00:00    Mouse             8.5            NaN        8.2\n",
       "331162  2017-12-31 20:00:00    Mouse             8.5            NaN        7.2\n",
       "331163  2017-12-31 21:00:00    Mouse             8.2            NaN       10.3\n",
       "331164  2017-12-31 22:00:00    Mouse             7.5            NaN       12.9\n",
       "331165  2017-12-31 23:00:00    Mouse             7.2            NaN       10.3\n",
       "\n",
       "[331166 rows x 5 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>timestamp</th>\n      <th>site_id</th>\n      <th>airTemperature</th>\n      <th>cloudCoverage</th>\n      <th>windSpeed</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2016-01-01 00:00:00</td>\n      <td>Panther</td>\n      <td>19.4</td>\n      <td>NaN</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2016-01-01 01:00:00</td>\n      <td>Panther</td>\n      <td>21.1</td>\n      <td>6.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2016-01-01 02:00:00</td>\n      <td>Panther</td>\n      <td>21.1</td>\n      <td>NaN</td>\n      <td>1.5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2016-01-01 03:00:00</td>\n      <td>Panther</td>\n      <td>20.6</td>\n      <td>NaN</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2016-01-01 04:00:00</td>\n      <td>Panther</td>\n      <td>21.1</td>\n      <td>NaN</td>\n      <td>1.5</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>331161</th>\n      <td>2017-12-31 19:00:00</td>\n      <td>Mouse</td>\n      <td>8.5</td>\n      <td>NaN</td>\n      <td>8.2</td>\n    </tr>\n    <tr>\n      <th>331162</th>\n      <td>2017-12-31 20:00:00</td>\n      <td>Mouse</td>\n      <td>8.5</td>\n      <td>NaN</td>\n      <td>7.2</td>\n    </tr>\n    <tr>\n      <th>331163</th>\n      <td>2017-12-31 21:00:00</td>\n      <td>Mouse</td>\n      <td>8.2</td>\n      <td>NaN</td>\n      <td>10.3</td>\n    </tr>\n    <tr>\n      <th>331164</th>\n      <td>2017-12-31 22:00:00</td>\n      <td>Mouse</td>\n      <td>7.5</td>\n      <td>NaN</td>\n      <td>12.9</td>\n    </tr>\n    <tr>\n      <th>331165</th>\n      <td>2017-12-31 23:00:00</td>\n      <td>Mouse</td>\n      <td>7.2</td>\n      <td>NaN</td>\n      <td>10.3</td>\n    </tr>\n  </tbody>\n</table>\n<p>331166 rows Ã— 5 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "# Weather\n",
    "weather = pd.read_csv(url_root + path_weather + \"weather.csv\", usecols=([\"timestamp\",\"site_id\"]+weather_cols))\n",
    "weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta data\n",
    "meta = pd.read_csv(\n",
    "    url_root + path_meta + \"metadata.csv\",\n",
    "    usecols=[\"building_id\",\"site_id\", \"electricity\", \"hotwater\", \"chilledwater\", \"water\", \"steam\", \"solar\", \"gas\", \"irrigation\"],)\n",
    "meta  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(lst1, lst2):\n",
    "    return list(set(lst1) & set(lst2))\n",
    "\n",
    "def union(list1, list2):\n",
    "    return list(set().union(list1, list2))\n",
    "\n",
    "def mergeAll(meter_df, weather_df, metadata_df, intersect_fct=intersection):\n",
    "    \"\"\"\"A function to merge meta, weather and meter data together.\n",
    "    The interstect_fct defines wether the merging is done using union of intersection ensembles.\"\"\"\n",
    "\n",
    "    # Extract upper level column (meter_type) information\n",
    "    meter_type_list = []\n",
    "    for meter_type, blg_id in meter_df.columns.values:\n",
    "        meter_type_list.append(meter_type)\n",
    "    meter_type_list = list(set(meter_type_list))\n",
    "\n",
    "    #  Identify only unique building ID within the meters considered\n",
    "    blg_dict = dict()\n",
    "    i = True\n",
    "    for meter in meter_type_list:\n",
    "        blg_dict[meter] = []\n",
    "        for blg_id in meter_df[meter].columns.values:\n",
    "            blg_dict[meter].append(blg_id)\n",
    "        if i:\n",
    "            blg_list_intersect = blg_dict[meter]\n",
    "            i = False\n",
    "        else:\n",
    "            blg_list_intersect = intersect_fct(blg_dict[meter], blg_list_intersect)\n",
    "\n",
    "    # Filters metadata with only current meter info & unique building intersection ids\n",
    "    site_list = []\n",
    "    for metername in meter_type_list:\n",
    "        df_meta = metadata_df.loc[np.logical_and(metadata_df[metername] == \"Yes\", metadata_df[\"building_id\"].isin(blg_list_intersect)),\n",
    "                                  [\"building_id\", \"site_id\"]].copy()\n",
    "        site_list.extend(list(df_meta.site_id.unique()))\n",
    "    site_list_unique = list(set(site_list))\n",
    "\n",
    "    # Filters weather with only current sites\n",
    "    df_weather = weather_df.loc[weather_df[\"site_id\"].isin(site_list_unique) == True,].copy()\n",
    "    # Converts timestamp to datetime object\n",
    "    df_weather[\"timestamp\"] = pd.to_datetime(df_weather[\"timestamp\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    dfs = []\n",
    "    for i in meter_type_list:\n",
    "        # Select only intersecting information within a set of buildings\n",
    "        df = pd.melt(meter_df[i][intersection(blg_dict[i], blg_list_intersect)].reset_index(),\n",
    "                      id_vars=\"timestamp\",\n",
    "                      var_name=\"building_id\",\n",
    "                      value_name=i)\n",
    "        df.set_index([\"building_id\", \"timestamp\"], inplace=True)\n",
    "        dfs.append(df)  # append to list\n",
    "    meter_df = pd.concat(dfs, axis=1)\n",
    "    del (dfs, df)\n",
    "\n",
    "    # Merge\n",
    "    meter_df = pd.merge(meter_df.reset_index(), df_meta, how=\"left\", on=\"building_id\").merge(\n",
    "        df_weather, how=\"left\", on=[\"timestamp\", \"site_id\"])\n",
    "    return meter_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    \"\"\"\"Function to reduce the memory usage of a dataframe.\n",
    "    Source: https://www.kaggle.com/caesarlupum/ashrae-start-here-a-gentle-introduction\"\"\"\n",
    "\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "source": [
    "## PreProcessing\n",
    "### Multidimensional cuboid selection - General formating"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Formating\n",
    "df_all = mergeAll(df_meter, weather, meta, intersect_fct=union)\n",
    "columns_considered = meter_data+weather_cols  # using all selected attributed\n",
    "\n",
    "# Unmelt - multicolumn frame {attributeX, building_id}\n",
    "df1 = df_all.pivot(index=\"timestamp\", columns=\"building_id\", values=columns_considered)\n",
    "# Reduce memory usage\n",
    "df1 = reduce_mem_usage(df1, verbose=True)\n",
    "df1"
   ]
  },
  {
   "source": [
    "### Multidimensional cuboid manipulation & transformation functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multicol_2ndColumnSelection(df_multicol, allcol1, col2):\n",
    "    \"\"\"\"Function to select data from a multi-column dataframe based on the 2nd column value.\n",
    "    From a defined 2nd-level column of interest - col2,\n",
    "     the function loops over the dataframe from all the values interest from the 1st-level column - allcol1\"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    for i in allcol1:\n",
    "        df[i] = df_multicol[i, col2].copy()\n",
    "    return df\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "def scale_NanRobust(data_array, scaler):\n",
    "    \"\"\" A function to scale an array while being robust to outliers.\n",
    "    Adapted from: https://stackoverflow.com/questions/55280054/handling-missing-nan-values-on-sklearn-preprocessing\"\"\"\n",
    "    # Set valid mask\n",
    "    nan_mask = np.isnan(data_array)\n",
    "    valid_mask = ~nan_mask\n",
    "    # create a result array\n",
    "    result = np.full(data_array.shape, np.nan)\n",
    "    # assign only valid cases to\n",
    "    result[valid_mask] = scaler.fit_transform(data_array[valid_mask].reshape(-1, 1)).reshape(data_array[valid_mask].shape)\n",
    "    return result\n",
    "\n",
    "def scale_df_columns_NanRobust(df_in, target_columns, scaler=MinMaxScaler(feature_range=(1, 2))):\n",
    "    \"\"\"\"A function to normalize columns of a dataframe per column, while being robust to Nan values.\n",
    "    The function returns a similar dataframe with missing values in identical places - normalized with the scaler object.\"\"\"\n",
    "    # Identify target from non-target column values\n",
    "    nontarget_columns = list(set(df_in.columns) - set(target_columns))\n",
    "    df = df_in[target_columns].copy()\n",
    "    # Scale over the target columns\n",
    "    array_scaled = []\n",
    "    for col in df.columns:\n",
    "        array_scaled.append(scale_NanRobust(df[col].values, scaler))\n",
    "    df_scaled = pd.DataFrame(np.vstack(array_scaled).transpose(), columns=df.columns)\n",
    "    # Set scaled dataframe index\n",
    "    df_scaled[df_in.index.name] = df_in.index\n",
    "    df_scaled.set_index([df_in.index.name], inplace=True, drop=True)\n",
    "    # Merge non-target columns to the scaled frame\n",
    "    df_scaled[nontarget_columns] = df_in[nontarget_columns]\n",
    "    return df_scaled"
   ]
  }
 ]
}
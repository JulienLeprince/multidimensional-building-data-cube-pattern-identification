{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Data Cleaning\n",
    "\n",
    "This script loads raw data from the Building-Data-Genome-Project-2 and performs data Cleaning."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Parameter Selection\n",
    "meter_data = [\"electricity\", \"gas\", \"hotwater\", \"chilledwater\"]\n",
    "\n",
    "# Path & url definition\n",
    "url_root = 'https://media.githubusercontent.com/media/buds-lab/building-data-genome-project-2/master/data/'\n",
    "path_meters = \"meters/raw/\"\n",
    "path_data_out = \"..\\\\data\\\\\"\n",
    "path_fig_out = \"..\\\\figures\\\\\"\n",
    "\n",
    "meter_files = [url_root + path_meters + meter+ \".csv\" for meter in meter_data]"
   ]
  },
  {
   "source": [
    "## Functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulativemissing_id(df_in, p_freq='H', cumulative_threshold=4):\n",
    "    \"\"\"\"Function to identify sections of cumulative missing values in a dataframe.\"\"\"\n",
    "\n",
    "    t_end, t_start = dict(), dict()\n",
    "    for col in df_in:\n",
    "        # Count number of consecutive NaNs in dataframe\n",
    "        # Source: https://stackoverflow.com/questions/29007830/identifying-consecutive-nans-with-pandas\n",
    "        df_NaNcount = pd.concat([df_in[col],\n",
    "                                 (df_in[col].isnull().astype(int)\n",
    "                                  .groupby(df_in[col].notnull().astype(int).cumsum())\n",
    "                                  .cumsum().to_frame('consec_count'))], axis=1)\n",
    "        # Loop over the cumulative missing values violating the threshold\n",
    "        t_end[col] = []\n",
    "        t_start[col] = []\n",
    "        t_MinusOne = 0\n",
    "        for t in df_NaNcount['consec_count'][df_NaNcount['consec_count'] >= cumulative_threshold].index:\n",
    "            # If next value of the ones exceeding the threshold == 0 then this one is the maximal one\n",
    "            try:\n",
    "                if df_NaNcount['consec_count'][t + pd.to_timedelta(1, unit=p_freq)] == 0:\n",
    "                    t_end[col].append(t)  # identified timestep of max cumulative missing values\n",
    "                    max_CumNaN = df_NaNcount['consec_count'][t]  # max cumulative missing values\n",
    "                    t_begin = t - pd.to_timedelta(max_CumNaN - 1,\n",
    "                                                  unit=p_freq)  # identified first timestep of cumulative missing values\n",
    "                    t_start[col].append(t_begin)  # Store values of t_start\n",
    "                    # df_NaNcount.loc[t_begin:t, 'consec_count'] = [max_CumNaN] * max_CumNaN  # Replacing NaN counts with max cumulated value\n",
    "            # If next value of the ones exceeding the threshold cannot be obtained (end of the dataframe) then the previous one is the maximum one\n",
    "            except:\n",
    "                t_end[col].append(t_MinusOne)  # identified timestep of max cumulative missing values\n",
    "                max_CumNaN = df_NaNcount['consec_count'][t_MinusOne]  # max cumulative missing values\n",
    "                t_begin = t_MinusOne - pd.to_timedelta(max_CumNaN - 1,\n",
    "                                              unit=p_freq)  # identified first timestep of cumulative missing values\n",
    "                t_start[col].append(t_begin)  # Store values of t_start\n",
    "            t_MinusOne = t\n",
    "    return t_start, t_end\n",
    "\n",
    "\n",
    "def gap_fill(df_in, t_start, t_end, p_freq='H'):\n",
    "    \"\"\"\"Hot-deck function - filling identified gaps from averaged surrounding week values.\"\"\"\n",
    "\n",
    "    # Define the time intervals where to drop data\n",
    "    for col in t_start:\n",
    "        itv_tot = pd.date_range(start=df_in[col].index[0], end=df_in[col].index[-1], freq=p_freq)\n",
    "        for i in range(len(t_start[col])):\n",
    "            shift_before_break = False\n",
    "            error = False\n",
    "            t_itv = pd.date_range(start=t_start[col][i], end=t_end[col][i], freq=p_freq)\n",
    "            # Identify the two time intervals before and after the missing values\n",
    "            nb_days = (t_end[col][i] - t_start[col][i]).days\n",
    "            shift = pd.to_timedelta(np.floor(nb_days)+1, unit=\"W\")\n",
    "            t_itv_before = pd.date_range(start=t_start[col][i] - shift, end=t_end[col][i] - shift, freq=p_freq)\n",
    "            # If NaN is value to replace -> shift from one more week\n",
    "            try:\n",
    "                if np.isnan(np.sum(df_in[col][t_itv_before].values)):\n",
    "                    shift_bef = shift\n",
    "                    while np.isnan(np.sum(df_in[col][t_itv_before].values)):\n",
    "                        shift_bef = shift_bef + pd.to_timedelta(1, unit=\"W\")\n",
    "                        if t_start[col][i] - shift_bef < df_in[col].index[0]:  # If the shift becomes out of bound\n",
    "                            shift_before_break = True\n",
    "                            break\n",
    "                        t_itv_before = pd.date_range(start=t_start[col][i] - shift_bef, end=t_end[col][i] - shift_bef, freq=p_freq)\n",
    "            except:\n",
    "                error = True\n",
    "                break\n",
    "            t_itv_after = pd.date_range(start=t_start[col][i] + shift, end=t_end[col][i] + shift, freq=p_freq)\n",
    "            try:\n",
    "                if np.isnan(np.sum(df_in[col][t_itv_after].values)):\n",
    "                    shift_after = shift\n",
    "                    while np.isnan(np.sum(df_in[col][t_itv_after].values)):\n",
    "                        shift_after = shift_after + pd.to_timedelta(1, unit=\"W\")\n",
    "                        if t_end[col][i] + shift_after > df_in[col].index[-1]:  # If the shift becomes out of bound\n",
    "                            t_itv_after = t_itv_before\n",
    "                            break\n",
    "                        t_itv_after = pd.date_range(start=t_start[col][i] + shift_after, end=t_end[col][i] + shift_after, freq=p_freq)\n",
    "            except:\n",
    "                error = True\n",
    "                break\n",
    "            if shift_before_break:  # If shift was out of bound do:\n",
    "                t_itv_before = t_itv_after\n",
    "                break\n",
    "            # Fill df_in with the average of the two\n",
    "            if ~error:\n",
    "                df_in[col][t_itv] = (df_in[col][t_itv_before].values + df_in[col][t_itv_after].values)/2\n",
    "    return df_in\n",
    "\n",
    "\n",
    "def rolling_window_fill(df_in, method='median', p_window=4):\n",
    "    \"\"\"\"A rolling window median/mean filling function.\n",
    "    p_window=2 - rolling window size (Should be less than 2 hours from C. Fan et al. 2015)\n",
    "    method='median'/'mean' - choses the method to use for filling\"\"\"\n",
    "\n",
    "    df_manip = df_in.copy()\n",
    "    for col in df_manip:\n",
    "    # calculate rolling median/mean\n",
    "        if method == 'median':\n",
    "            rm = df_manip[col].rolling(window=p_window, center=True).median()\n",
    "        elif method == 'mean':\n",
    "            rm = df_manip[col].rolling(window=p_window, center=True).mean()\n",
    "        df_manip[col] = rm.ffill(limit=int(p_window/2+1)).bfill(limit=int(p_window/2+1)).interpolate(method='time', axis=0)\n",
    "    return df_manip"
   ]
  },
  {
   "source": [
    "## Data Cleaning\n",
    "### Outlier detection\n",
    "A Hampel filter moving window method is applied to detect point-wise outliers with a window size of 6 time-steps (hours) and a standard-deviation threshold of 3.\n",
    "### Missing value filling\n",
    "A single imputation approach is chosen in function of the length of the missing sections:\n",
    " - A moving average regression is chosen for missing data sections smaller than 3 consecutive hours,\n",
    " - Longer sections rely on the hot deck method, where missing values are averaged from identical time intervals and day of the week using sections of 2 weeks."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in meter_files:\n",
    "    # Read\n",
    "    meter_type = f.split(\"/\")[10].split(\".\")[0]\n",
    "    df = pd.read_csv(f, index_col=\"timestamp\")\n",
    "    df.index = pd.to_datetime(df.index, format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    # Drop columns with more than 60% of missing values\n",
    "    df = df.loc[:, df.isna().mean() < .6]\n",
    "\n",
    "    # Outlier detection - Hampfel filter\n",
    "    # for col in df_cub1.columns:\n",
    "    #     df_cub1[col] = od.hampel_filter_pandas(df_cub1[col], window_size=4, n_sigmas=5) # Bug reported to pandas, with np.argwhere function, should be fixed in an upcoming patch\n",
    "\n",
    "    # Missing value filling\n",
    "    # Identifying missing value gaps within the dataframe\n",
    "    t_start, t_end = cumulativemissing_id(df, p_freq='H', cumulative_threshold=4)\n",
    "    # Hot-deck gap filling method\n",
    "    df = gap_fill(df, t_start, t_end, p_freq='H')\n",
    "    # Interpolate leftover gaps\n",
    "    df = rolling_window_fill(df, method='mean', p_window=4)\n",
    "\n",
    "    # Write results\n",
    "    df.to_csv(path_data_out+meter_type+'.csv')"
   ]
  }
 ]
}
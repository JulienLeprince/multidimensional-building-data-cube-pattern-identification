{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multidimensional Pattern identification with SAX\n",
    "## Building Benchmarking\n",
    "\n",
    "This script performs pattern identification over the {time, site} cuboid.\n",
    "\n",
    "The data is first normalized then transformed using SAX over normalized daily sequences. Motifs are identified across buildings, and a final clustering phase is executed over the reduced counts of sequences. \n",
    "\n",
    "Results are presented visually allowing interpretable analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.cluster import KMeans\n",
    "import sklearn.metrics as metrics\n",
    "# SAX package - source https://github.com/seninp/saxpy\n",
    "from saxpy.alphabet import cuts_for_asize\n",
    "from saxpy.znorm import znorm\n",
    "from saxpy.sax import ts_to_string\n",
    "from saxpy.paa import paa\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Plotting modules\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcdefaults()\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objects as go\n",
    "from plotly.colors import n_colors\n",
    "from plotly.subplots import make_subplots\n",
    "from plotly.offline import init_notebook_mode\n",
    "init_notebook_mode(connected = True)\n",
    "\n",
    "# Version\n",
    "version = \"v1.0\"\n",
    "\n",
    "# Path definition\n",
    "path_data = \"..\\\\data\\\\cube\\\\\"\n",
    "path_fig_out = \"..\\\\figures\\\\\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading\n",
    "\n",
    "Meter data are organized in multicolumn dataframes with {meter, building_id} as column keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    \"\"\"\"Function to reduce the memory usage of a dataframe.\n",
    "    Source: https://www.kaggle.com/caesarlupum/ashrae-start-here-a-gentle-introduction\"\"\"\n",
    "\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Cuboid\n",
    "df_cubA = pd.read_csv(path_data + \"cuboid_C_hereissometimestamp.csv\", index_col=\"timestamp\")\n",
    "buildings = df_cubA.columns.values\n",
    "\n",
    "# Reduce memory usage\n",
    "df = reduce_mem_usage(df_cubA, verbose=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Mining\n",
    "### Multidimensional cuboid manipulation & transformation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multicol_2ndColumnSelection(df_multicol, allcol1, col2):\n",
    "    \"\"\"\"Function to select data from a multi-column dataframe based on the 2nd column value.\n",
    "    From a defined 2nd-level column of interest - col2,\n",
    "     the function loops over the dataframe from all the values interest from the 1st-level column - allcol1\"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    for i in allcol1:\n",
    "        df[i] = df_multicol[i, col2].copy()\n",
    "    return df\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "def scale_NanRobust(data_array, scaler):\n",
    "    \"\"\" A function to scale an array while being robust to outliers.\n",
    "    Adapted from: https://stackoverflow.com/questions/55280054/handling-missing-nan-values-on-sklearn-preprocessing\"\"\"\n",
    "    # Set valid mask\n",
    "    nan_mask = np.isnan(data_array)\n",
    "    valid_mask = ~nan_mask\n",
    "    # create a result array\n",
    "    result = np.full(data_array.shape, np.nan)\n",
    "    # assign only valid cases to\n",
    "    result[valid_mask] = scaler.fit_transform(data_array[valid_mask].reshape(-1, 1)).reshape(data_array[valid_mask].shape)\n",
    "    return result\n",
    "\n",
    "def scale_df_columns_NanRobust(df_in, target_columns, scaler=MinMaxScaler(feature_range=(1, 2))):\n",
    "    \"\"\"\"A function to normalize columns of a dataframe per column, while being robust to Nan values.\n",
    "    The function returns a similar dataframe with missing values in identical places - normalized with the scaler object.\"\"\"\n",
    "    # Identify target from non-target column values\n",
    "    nontarget_columns = list(set(df_in.columns) - set(target_columns))\n",
    "    df = df_in[target_columns].copy()\n",
    "    # Scale over the target columns\n",
    "    array_scaled = []\n",
    "    for col in df.columns:\n",
    "        array_scaled.append(scale_NanRobust(df[col].values, scaler))\n",
    "    df_scaled = pd.DataFrame(np.vstack(array_scaled).transpose(), columns=df.columns)\n",
    "    # Set scaled dataframe index\n",
    "    df_scaled[df_in.index.name] = df_in.index\n",
    "    df_scaled.set_index([df_in.index.name], inplace=True, drop=True)\n",
    "    # Merge non-target columns to the scaled frame\n",
    "    df_scaled[nontarget_columns] = df_in[nontarget_columns]\n",
    "    return df_scaled\n",
    "\n",
    "def multi2singlecol_1stCol(df_in):\n",
    "    \"\"\"\"Function to transform a 2 column dataframe to a single one, while appending the 2nd column information\n",
    "    to a new attribute.\"\"\"\n",
    "    # Extract upper level column meter_type information\n",
    "    meter_type_list = []\n",
    "    for meter_type, blg_id in df_in.columns.values:\n",
    "        meter_type_list.append(meter_type)\n",
    "    meter_type_list = list(set(meter_type_list))\n",
    "\n",
    "    dfs = []\n",
    "    for i in meter_type_list:\n",
    "        df1 = pd.melt(df_in[i].reset_index(),\n",
    "                      id_vars=df_in.index.name,\n",
    "                      var_name=\"building_id\",\n",
    "                      value_name=i)\n",
    "        df1.set_index([\"building_id\", df_in.index.name], inplace=True)\n",
    "        dfs.append(df1)  # append to list\n",
    "    meter_df = pd.concat(dfs, axis=1)\n",
    "    meter_df = meter_df.reset_index().set_index([df_in.index.name], drop=True)\n",
    "    return meter_df\n",
    "\n",
    "def checkIfExists(elem):\n",
    "    if elem:\n",
    "        return elem\n",
    "    else:\n",
    "        return ['None']\n",
    "\n",
    "def flatten_list(l):\n",
    "    for el in l:\n",
    "        if isinstance(el, list):\n",
    "            yield from flatten_list(el)\n",
    "        else:\n",
    "            yield el"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pattern Identification\n",
    "## Building Benchmarking\n",
    "Cuboid {time, site} selection, shows an inter-building analytical frame, typically relevant for cross building benchmarking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAX functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SAX_mining(df_in, W=4, A=3):\n",
    "    \"\"\"\"Function to perform daily SAX mining on input dataframe\"\"\"\n",
    "    # Input definition of the function\n",
    "    df_sax = df_in.copy()\n",
    "    df_sax['Day'] = df_sax.index.dayofyear\n",
    "    df_sax['Hour'] = df_sax.index.hour\n",
    "\n",
    "    # Daily SAX over the year with reduced daily size\n",
    "    sax_dict, counts, sax_data = dict(), dict(), dict()\n",
    "    for meter in df_in.columns.values:\n",
    "        # Daily heatmaps over all year\n",
    "        sax_data[meter] = pd.pivot_table(df_sax, values=meter,\n",
    "                                  index=['Day'], columns='Hour')\n",
    "        sax_data[meter] = reduce_mem_usage(sax_data[meter], verbose=False)\n",
    "\n",
    "        # Daily SAX obtained here with hourly resolution\n",
    "        daily_sax = []\n",
    "        for i in range(sax_data[meter].shape[0]):\n",
    "            dat_paa = paa(sax_data[meter].values[i], W)\n",
    "            daily_sax.append(ts_to_string(dat_paa, cuts_for_asize(A)))\n",
    "        sax_dict[meter] = daily_sax\n",
    "\n",
    "        # Now count the number of similar elements in the SAX list\n",
    "        counts[meter] = Counter(sax_dict[meter])\n",
    "    return sax_dict, counts, sax_data\n",
    "\n",
    "def sax_count_reformat(sax_dict):\n",
    "    \"\"\"\"Function to format SAX counts to a unified dataframe.\"\"\"\n",
    "    df_concat = [] #pd.DataFrame(columns=df_cub2.columns)\n",
    "    for meter_data in sax_dict:\n",
    "        counts[meter_data] = Counter(sax_dict[meter_data])\n",
    "        # Create a dataframe from the counter object\n",
    "        df_concat.append(pd.DataFrame.from_dict(counts[meter_data], orient='index', columns=[meter_data]))\n",
    "    # Now concatenate the dictionary to one dataframe\n",
    "    df_count = pd.concat(df_concat, axis=1)  # Reformated dataframe\n",
    "    return df_count\n",
    "\n",
    "def SAXcount_hm_wdendro(df_count, title):\n",
    "    # Create Side Dendrogram\n",
    "    # source: https://plotly.com/python/dendrogram/\n",
    "    dendo = ff.create_dendrogram(df_count.values, orientation='left', labels=list(df_count.index.values))\n",
    "    for i in range(len(dendo['data'])):\n",
    "        dendo['data'][i]['xaxis'] = 'x2'\n",
    "    # Create Heatmap\n",
    "    dendro_leaves_txt = dendo['layout']['yaxis']['ticktext']\n",
    "    # Convert the txt leaves to integer index values\n",
    "    dendro_leaves = []\n",
    "    for txt in df_count.index.values:\n",
    "        dendro_leaves.append(list(dendro_leaves_txt).index(txt))\n",
    "    dendro_leaves = list(map(int, dendro_leaves))\n",
    "\n",
    "    heat_data = df_count.values[dendro_leaves, :]\n",
    "\n",
    "    # Calling the subplots\n",
    "    fig = go.Figure(data=go.Heatmap(z=heat_data,\n",
    "                                    x=df_count.columns,\n",
    "                                    y=df_count.index[dendro_leaves],\n",
    "                                    # zmax=pzmax, zmin=pzmin,\n",
    "                                    colorbar={\"title\": \"Counts\"},\n",
    "                                    colorscale='Blues'))\n",
    "    p_width = len(df_count.columns)*5 if len(df_count.columns)*5 > 400 else 400\n",
    "    fig.update_layout(height=900, width=p_width,\n",
    "                      xaxis={\"tickmode\": \"array\"},\n",
    "                      title_text=f\"SAX counts for attribute: {title}\",\n",
    "                      plot_bgcolor='#fff'\n",
    "                      )\n",
    "    fig.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# Fix attribute\n",
    "attribute = columns_considered[0]\n",
    "\n",
    "## Cuboid selection from the data-cube-frame\n",
    "df_cub1 = df[attribute]\n",
    "df_cub1 = reduce_mem_usage(df_cub1, verbose=False)\n",
    "df_cub1.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "## Data manipulation to get daily SAX\n",
    "df_sax = df_cub1.copy()\n",
    "\n",
    "# SAX Parameters\n",
    "day_number_of_pieces = 4\n",
    "alphabet_size = 3\n",
    "scaler_function = StandardScaler()\n",
    "\n",
    "# Normalize per attribute robust to Nans\n",
    "df_sax_normalized = scale_df_columns_NanRobust(df_sax, df_sax.columns, scaler=scaler_function)\n",
    "\n",
    "# SAX transformation\n",
    "sax_dict, counts, sax_data = SAX_mining(df_sax_normalized, W=day_number_of_pieces, A=alphabet_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting sequence counts as a heatmap w/ dendrogram organization\n",
    "df_count = sax_count_reformat(sax_dict)\n",
    "df_count.fillna(0, inplace=True)\n",
    "fig = SAXcount_hm_wdendro(df_count, attribute)\n",
    "fig.write_image(path_fig_out+\"building_bench/SAXcounts_\"+attribute+\"_\"+version+\".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reformating daily sax results for plotting all daily SAX\n",
    "# sax_dict_data, index_map_dictionary = dict(), dict()\n",
    "# for meter in sax_data:\n",
    "#     sax_dict_data[meter], index_map_dictionary[meter] = sax_df_reformat(sax_data, sax_dict, meter)\n",
    "# # Plotting all daily SAX and saving figure\n",
    "# fig = SAX_dailyhm_visualization(sax_dict_data, sax_dict, index_map_dictionary, lattice_selected)\n",
    "# fig.write_image(path_out+\"SAX_attrib_\"+lattice_selected+\"_\"+version+\".png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-building clustering\n",
    "We proceed to perform cross-site clustering based on motif similarities between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbow_wcss(X, n_cluster_max):\n",
    "    \"\"\"\"Within Cluster Sum of Squares (WCSS) method for optimal number of clusters identification\"\"\"\n",
    "    wcss, sil = [], []  # Within Cluster Sum of Squares (WCSS) & silhouette index\n",
    "    for i in range(2, n_cluster_max):\n",
    "        kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "        kmeans_pred_y = kmeans.fit_predict(X)\n",
    "        wcss.append(kmeans.inertia_)  # WCSS\n",
    "        sil.append(metrics.silhouette_score(X, kmeans_pred_y, metric=\"euclidean\"))  # Silhouette score\n",
    "    return wcss, sil\n",
    "\n",
    "def similarity_index_plot(wcss, sil):\n",
    "    fig, axs = plt.subplots(2, 1, sharex=True)\n",
    "    axs[0].scatter(range(2,len(sil)+2),sil,\n",
    "                   c='c', marker='v')\n",
    "    axs[1].scatter(range(2,len(wcss)+2),wcss,\n",
    "                   c='r', marker='o')\n",
    "    # Legend\n",
    "    axs[0].set_ylabel('Silhouette')\n",
    "    axs[1].set_ylabel('Cluster Sum of Squares')\n",
    "    axs[1].set_xlabel('cluster number')\n",
    "    # Set ticks inside\n",
    "    plt.xticks(range(2,len(sil)+2), range(2,len(sil)+2))\n",
    "    axs[0].tick_params(axis=\"y\", direction=\"in\", left=\"off\", labelleft=\"on\")\n",
    "    axs[0].tick_params(axis=\"x\", direction=\"in\", left=\"off\", labelleft=\"on\")\n",
    "    axs[1].tick_params(axis=\"x\", direction=\"in\", left=\"off\", labelleft=\"on\")\n",
    "    axs[1].tick_params(axis=\"y\", direction=\"in\", left=\"off\", labelleft=\"on\")\n",
    "    axs[0].grid(axis='y', color='grey', linestyle='--', linewidth=0.5, alpha=0.4)\n",
    "    axs[1].grid(axis='y', color='grey', linestyle='--', linewidth=0.5, alpha=0.4)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "def cluster_counter_plot(counts, title=None):\n",
    "    \"\"\"Plot motif counts per cluster with whiskers\"\"\"\n",
    "    stats = counts.describe()\n",
    "    stats = stats.transpose().sort_values(by=['50%'], ascending=False).transpose()\n",
    "    keys = counts.columns\n",
    "    y_pos = np.arange(len(keys))\n",
    "    yerr_pos = stats.loc['75%'].values - stats.loc['50%'].values\n",
    "    yerr_neg = stats.loc['50%'].values - stats.loc['25%'].values\n",
    "    # Now plotting\n",
    "    fig = plt.figure(figsize=(6, 4))\n",
    "    ax = plt.gca()\n",
    "    plt.bar(y_pos,\n",
    "             stats.loc['50%'].values,\n",
    "             #yerr=stats.loc[['75%','25%']].values,          # Define error bar as the difference between 50% and quantiles\n",
    "             yerr = [yerr_neg, yerr_pos],\n",
    "             tick_label=keys,\n",
    "             align='center',\n",
    "             alpha=0.4)\n",
    "    hfont = {'fontname': 'Times New Roman'}\n",
    "    ax.tick_params(axis=\"y\", direction=\"in\", left=\"off\", labelleft=\"on\", labelsize=13)\n",
    "    ax.tick_params(axis=\"x\", direction=\"in\", left=\"off\", labelleft=\"on\", labelsize=13)\n",
    "    plt.xticks(y_pos, keys, rotation=90, **hfont)\n",
    "    plt.yticks(**hfont)\n",
    "    #plt.xlabel('Symbolic Aggregate Approximation sequences', **hfont)\n",
    "    plt.ylabel('counts', fontsize=15, **hfont)\n",
    "    plt.title(title, **hfont)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clustering identified motifs\n",
    "\n",
    "# Filter discords from established threshold\n",
    "motif_threshold_number = 5\n",
    "df_count_motifs = df_count[df_count > motif_threshold_number]\n",
    "df_count_motifs.fillna(0, inplace=True)\n",
    "\n",
    "# Identify optimal cluster number\n",
    "wcss, sil = elbow_wcss(df_count_motifs.transpose(), n_cluster_max=20)\n",
    "\n",
    "# Plots\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "plt.rcParams['font.sans-serif'] = ['Times New Roman']\n",
    "fig = similarity_index_plot(wcss, sil)\n",
    "fig.savefig(path_fig_out+\"building_bench/CrossBlg_\"+attribute+\"_cluster_SimilarityIndex_\"+version+\".jpg\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster identified motifs\n",
    "nb_clusters_opt = 6\n",
    "kmeans = KMeans(n_clusters=nb_clusters_opt, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "kmeans_pred_y = kmeans.fit_predict(df_count_motifs.transpose())\n",
    "\n",
    "# Identify buildings & motifs per cluster\n",
    "blg_clus, motifs, count_clustmotifs = dict(), dict(), dict()\n",
    "for clus in range(nb_clusters_opt):\n",
    "    # Identifying buildings within the cluster\n",
    "    blg_clus[clus] = df_count_motifs.transpose()[kmeans_pred_y == clus].index.values\n",
    "    # Extracting motifs within the cluster without empty columns (only zeros)\n",
    "    dfc = df_count_motifs[blg_clus[clus]].replace(0, np.nan)\n",
    "    dfc = dfc.dropna(how='all', axis=0)\n",
    "    dfc = dfc.replace(np.nan, 0)\n",
    "    motifs[clus] = dfc.index.values\n",
    "    # Extracting motifs counts within cluster\n",
    "    count_clustmotifs[clus] = df_count_motifs[blg_clus[clus]].transpose()[motifs[clus]]\n",
    "\n",
    "# Plot counts per cluster as bar plots with whisker margins\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "for clus in count_clustmotifs:\n",
    "    fig = cluster_counter_plot(count_clustmotifs[clus], title=\"cluster \" + str(clus) +\" (N=\"+str(len(blg_clus[clus]))+\")\")\n",
    "    fig.savefig(path_fig_out+\"building_bench/SAXclust_\"+attribute+\"_clust_\"+str(clus)+\"_\"+version+\".jpg\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-site view\n",
    "Cuboid {time, attribute} selection covers the intra-building frame. It serves for within-site exploration on how a given building operates across time and building-specific attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counter_plot(counter, title=None):\n",
    "    \"\"\"Simple demo of a horizontal bar chart.\n",
    "    Source: https://stackoverflow.com/questions/22222573/how-to-plot-counter-object-in-horizontal-bar-chart\"\"\"\n",
    "    # Sort the counter dictionnary per value\n",
    "    # source: https://stackoverflow.com/questions/613183/how-do-i-sort-a-dictionary-by-value\n",
    "    counter = {k: v for k, v in sorted(counter.items(), key=lambda item: item[1])}\n",
    "    # Counter data, counter is your counter object\n",
    "    keys = counter.keys()\n",
    "    y_pos = np.arange(len(keys))\n",
    "    # get the counts for each key, assuming the values are numerical\n",
    "    performance = [counter[k] for k in keys]\n",
    "    # Now plotting\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    plt.barh(y_pos, performance, align='center', alpha=0.4)\n",
    "    plt.yticks(y_pos, keys)\n",
    "    plt.xlabel('Number of profiles per symbolic sequence')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "def sax_df_reformat(sax_data, sax_dict, meter_data, space_btw_saxseq=3):\n",
    "    \"\"\"\"Function to format a SAX timeseries original data for SAX heatmap plotting.\"\"\"\n",
    "\n",
    "    counts[meter_data] = Counter(sax_dict[meter_data])\n",
    "    # Sort the counter dictionnary per value\n",
    "    # source: https://stackoverflow.com/questions/613183/how-do-i-sort-a-dictionary-by-value\n",
    "    counter = {k: v for k, v in sorted(counts[meter_data].items(), key=lambda item: item[1])}\n",
    "    keys = counter.keys()\n",
    "\n",
    "    empty_sax_df = pd.DataFrame(columns=sax_data[meter_data].columns, index=[' ']*space_btw_saxseq)\n",
    "    new_sax_df = pd.DataFrame(columns=sax_data[meter_data].columns)\n",
    "    for sax_seq in keys:\n",
    "        # Obtaining sax indexes of corresponding profiles within dataframe\n",
    "        indexes = [i for i,x in enumerate(sax_dict[meter_data]) if x == sax_seq]   # returns all indexes\n",
    "        # Formating a newdataframe from selected sax_seq\n",
    "        df_block = sax_data[meter_data].iloc[indexes].copy()\n",
    "        df_block[\"SAX\"] = [sax_seq]*len(indexes)\n",
    "        new_sax_df = pd.concat([df_block, empty_sax_df, new_sax_df], axis=0) # Reformated dataframe\n",
    "    # Mapping the sax sequence to the data\n",
    "    index_map_dictionary = dict()\n",
    "    index_map_dictionary[\"SAX_seq\"], index_map_dictionary[\"SAX_idx\"] = [], []\n",
    "    for sax_seq in keys:\n",
    "        indexes = [i for i, x in enumerate(new_sax_df[\"SAX\"]) if x == sax_seq]  # returns all indexes\n",
    "        index_map_dictionary[\"SAX_seq\"].append(sax_seq)\n",
    "        index_map_dictionary[\"SAX_idx\"].append(np.median(indexes))\n",
    "    # Droping the SAX column of the dataframe now that we have a mapping variable for it\n",
    "    new_sax_df.drop(\"SAX\", axis=1, inplace=True)\n",
    "    return new_sax_df, index_map_dictionary\n",
    "\n",
    "def SAX_dailyhm_visualization(dict_numeric, sax_dict, index_map_dictionary, title):\n",
    "    \"\"\"\"Function to visualize multi-attribute SAX sequences from original dataframe with SAX dictionary\"\"\"\n",
    "    keys = list(sax_dict.keys())\n",
    "    key_int = 0\n",
    "    # First loop over all keys and data to identify min and max values of the time series\n",
    "    for key in sax_dict:    # key can be meter_data or bld_id depending on the cuboid selected\n",
    "        sax_seq_int = 0\n",
    "        for sax_sequence_toidentify in sax_dict[key]:\n",
    "            indexes = [i for i, x in enumerate(sax_dict[key]) if x == sax_sequence_toidentify]  # returns all indexes\n",
    "\n",
    "            if key_int < 1:\n",
    "                pzmax = dict_numeric[key].iloc[indexes].max().max()\n",
    "                pzmin = dict_numeric[key].iloc[indexes].min().min()\n",
    "            else:\n",
    "                pzmax = max(pzmax, dict_numeric[key].iloc[indexes].max().max())\n",
    "                pzmin = min(pzmin, dict_numeric[key].iloc[indexes].min().min())\n",
    "            sax_seq_int = sax_seq_int + 1\n",
    "        key_int = key_int + 1\n",
    "\n",
    "\n",
    "    # Calling the subplots\n",
    "    fig = make_subplots(rows=1, cols=len(keys), shared_yaxes=False, \n",
    "    horizontal_spacing=0.01+len(sax_dict[key][0])*0.005, column_titles=keys, x_title=\"Hour of the day\")\n",
    "    # Then Loop again of the set to plot\n",
    "    key_int = 0\n",
    "    # Looping over sax keys (i.e. attributes or blg keys)\n",
    "    for key in sax_dict:\n",
    "        # Plot\n",
    "        fig.add_trace(go.Heatmap(z=dict_numeric[key],\n",
    "                                 x=dict_numeric[key].columns,\n",
    "                                 zmax=pzmax, zmin=pzmin,\n",
    "                                 colorbar={\"title\": \"Attribute normalized value\"},\n",
    "                                 colorscale='temps'),\n",
    "                      row=1, col=key_int + 1)\n",
    "        fig.update_yaxes(tickmode='array',\n",
    "                         tickvals=index_map_dictionary[key][\"SAX_idx\"],\n",
    "                         ticktext=index_map_dictionary[key][\"SAX_seq\"],\n",
    "                         row=1, col=key_int+1)\n",
    "        key_int = key_int + 1\n",
    "    fig.update_layout(height=800, width=len(keys)*250,\n",
    "                      xaxis={\"tickmode\": \"array\"},\n",
    "                      title_text=f\"Daily SAX profiles of {title}\",\n",
    "                      plot_bgcolor='#fff'\n",
    "                      )\n",
    "    fig.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fix building identification\n",
    "blg_id = \"Bobcat_education_Alissa\"\n",
    "\n",
    "## Data manipulation\n",
    "df_cub2 = multicol_2ndColumnSelection(df, columns_considered, blg_id)\n",
    "df_cub2.dropna(axis=1, how='all', inplace=True)\n",
    "lattice_selected = blg_id\n",
    "df_sax = df_cub2.copy()\n",
    "\n",
    "# Normalize per attribute\n",
    "df_sax_normalized = scale_df_columns_NanRobust(df_sax, df_sax.columns, scaler=scaler_function)\n",
    "df_sax_normalized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform SAX transformation\n",
    "sax_dict, counts, sax_data = SAX_mining(df_sax_normalized, W=day_number_of_pieces, A=alphabet_size)\n",
    "\n",
    "# Plot the sequence counts per attribute\n",
    "for meter in df_cub2.columns.values:\n",
    "    fig = counter_plot(counts[meter], title=meter)\n",
    "    fig.savefig(path_fig_out+\"insite_view/SAXcounts_Attrib_StandardScaler_blg_\"+blg_id+\"_meter_\"+meter+\"_\"+version+\".jpg\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformating sax results for plotting\n",
    "sax_dict_data, index_map_dictionary = dict(), dict()\n",
    "for meter in sax_data:\n",
    "    sax_dict_data[meter], index_map_dictionary[meter] = sax_df_reformat(sax_data, sax_dict, meter)\n",
    "\n",
    "# Plotting and saving figure\n",
    "fig = SAX_dailyhm_visualization(sax_dict_data, sax_dict, index_map_dictionary, lattice_selected)\n",
    "fig.write_image(path_fig_out+\"insite_view/SAX_blg_\"+lattice_selected+\"_\"+version+\".png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attribute motifs clustering\n",
    "Attribute daily profile motifs are clustered together resulting in a reduced number of typical patterns from the previous motif identification thanks to SAX trasnformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clustering identified motifs\n",
    "\n",
    "# Filter discords from established threshold\n",
    "threshold = 10  # motif number threshold\n",
    "indexes = dict()\n",
    "for meter in df_cub2.columns.values:\n",
    "    df_count = pd.DataFrame.from_dict(Counter(sax_dict[meter]), orient='index').rename(columns={0:'count'})\n",
    "    motifs = df_count[(df_count > threshold)]\n",
    "    indexes[meter] = [i for i,x in enumerate(sax_dict[meter]) if x in list(motifs.index)]  # returns all indexes\n",
    "\n",
    "# Cluster the identified motifs\n",
    "nb_clusters_opt = 5\n",
    "kmeans = KMeans(n_clusters=nb_clusters_opt, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "kmeans_pred_y, clust_sax_data = dict(), dict()\n",
    "for meter in df_cub2.columns.values:\n",
    "    clust_sax_data[meter] = sax_data[meter].iloc[indexes[meter]]\n",
    "    kmeans_pred_y[meter] = kmeans.fit_predict(clust_sax_data[meter].interpolate(method='linear'))\n",
    "\n",
    "# Reformating cluster results for plotting\n",
    "clust_dict_data, index_map_dictionary = dict(), dict()\n",
    "max_shape = 0\n",
    "for meter in sax_data:\n",
    "    clust_dict_data[meter], index_map_dictionary[meter] = sax_df_reformat(clust_sax_data, kmeans_pred_y, meter)\n",
    "    max_shape = max(max_shape, max(np.shape(clust_dict_data[meter])))\n",
    "# Adjusting reformaating from variable attribute motifs lengths\n",
    "for meter in sax_data:\n",
    "    # Defining width of empty dataframe to add\n",
    "    space_btw_saxseq = max_shape - max(np.shape(clust_dict_data[meter]))\n",
    "    # Creating empty frame\n",
    "    empty_sax_df = pd.DataFrame(columns=sax_data[meter].columns, index=[' ']*space_btw_saxseq)\n",
    "    # Adding empty frame to the df\n",
    "    clust_dict_data[meter] = clust_dict_data[meter].append(empty_sax_df)\n",
    "\n",
    "# Plotting cluster results results\n",
    "fig = SAX_dailyhm_visualization(clust_dict_data, sax_dict, index_map_dictionary, title=blg_id)\n",
    "fig.write_image(path_fig_out+\"insite_view/clust_blg_\"+blg_id+\"_\"+version+\".png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross building/attribute slice\n",
    "The {site, attribute} cuboid allows exploration of cross-building/attributes combined analysis within a fixed time slice of interested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SAXannotated_heatmap_viz(df_in, df_text, title):\n",
    "    fig = ff.create_annotated_heatmap(z=df_in.values.tolist(),\n",
    "                                      x=df_in.columns.values.tolist(),\n",
    "                                      y=df_in.index.values.tolist(),\n",
    "                                      annotation_text=df_text,\n",
    "                                      colorbar={\"title\": \"Counts\"},\n",
    "                                      colorscale='Blues')\n",
    "    p_height = len(df_in.index)*15 if len(df_in.index)*15 > 400 else 400\n",
    "    p_width = len(df_in.columns)*120 if len(df_in.columns)*120 > 400 else 400\n",
    "    fig.update_layout(height=p_height, width=p_width,\n",
    "                      xaxis={\"tickmode\": \"array\"},\n",
    "                      #title_text=f\" Daily SAX for time slice: {title}\",\n",
    "                      plot_bgcolor='#fff'\n",
    "                      )\n",
    "    fig.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Per timerange insight\n",
    "start_date = \"2017-01-01 00:00:00\"\n",
    "end_date = \"2017-01-01 23:00:00\"\n",
    "timestamp_considered = \"2017-01-01\"\n",
    "\n",
    "## OLAP - Daily averaging\n",
    "df_d = df.copy().resample('D').mean()\n",
    "timerange_considered = (df_d.index >= start_date) & (df_d.index <= end_date)\n",
    "df_d = df_d.loc[timerange_considered].transpose().reset_index()\n",
    "df_d.columns = [\"meter\", \"building_id\", \"value\"]\n",
    "df_cub3olap = df_d.pivot(index=\"meter\", columns=\"building_id\", values=\"value\")\n",
    "df_cub3olap = df_cub3olap.dropna(axis=1, how='all').transpose()\n",
    "# Normalize per column (attribute)\n",
    "scaler_function = MinMaxScaler() #StandardScaler()\n",
    "df_olap_normalized = scale_df_columns_NanRobust(df_cub3olap, target_columns=df_cub3olap.columns, scaler=scaler_function)\n",
    "\n",
    "## OLAM - SAX transformation\n",
    "timerange_considered = (df.index >= start_date) & (df.index <= end_date)\n",
    "df_cub3 = df.loc[timerange_considered]\n",
    "df_cub3.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "## Data manipulation to get daily SAX\n",
    "# Attribute data is scaled across sites\n",
    "df_sax = multi2singlecol_1stCol(df_cub3)\n",
    "\n",
    "# Normalize per column (here attribute) robust to Nans\n",
    "columns_considered = [col for col in df_sax.columns if col not in ['building_id', 'timestamp']]\n",
    "df_sax_normalized = scale_df_columns_NanRobust(df_sax, target_columns=columns_considered, scaler=scaler_function)\n",
    "\n",
    "# Now loop over unmerged column to perform SAX\n",
    "sax_dict, counts, sax_data = dict(), dict(), dict()\n",
    "dfs = []\n",
    "for blg in df_sax_normalized['building_id'].unique():\n",
    "    sax_dict[blg], counts[blg], sax_data[blg] = SAX_mining(df_sax_normalized[columns_considered][df_sax_normalized['building_id'] == blg],\n",
    "                                                           W=day_number_of_pieces, A=alphabet_size)\n",
    "    # Append the results to a list\n",
    "    dfs.append(pd.DataFrame(sax_dict[blg].items(), columns=['Meter', blg]).set_index('Meter'))\n",
    "# Concatenate the list to a common dataframe\n",
    "df_sax_dict = pd.concat(dfs, axis=1)\n",
    "\n",
    "# Post-processing\n",
    "for col in df_sax_dict.columns:\n",
    "    df_sax_dict[col] = list(flatten_list([[subitem for subitem in checkIfExists(item)] for item in df_sax_dict[col].values]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cross-building/attribute SAX heatmap\n",
    "title = timestamp_considered\n",
    "# Re-adjusting index orders\n",
    "# df_in = df_cub3_normalized.transpose().sort_index(axis=0)\n",
    "# df_text = df_sax_dict.sort_index(axis=0).values.tolist()\n",
    "# Pivoting the dataframe for plotting\n",
    "df_in = df_olap_normalized.sort_index(axis=1)\n",
    "df_text = df_sax_dict.transpose().sort_index(axis=1).values.tolist()\n",
    "# Plot Heatmap\n",
    "fig = SAXannotated_heatmap_viz(df_in, df_text, title)\n",
    "fig.write_image(path_fig_out+\"cross_blgattrib_slice/SAX_time_\"+timestamp_considered+\"_\"+version+\".png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross building/attribute clustering\n",
    "Buildings are clustered together from an within time slice view of the data cube. This time multi-attributes are used with clustering leveraging high-dimensional similarity metrics. Here notions of motifs and discords cannot be exploited as a result of the limited time-slice view only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}